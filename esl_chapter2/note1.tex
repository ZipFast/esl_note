\documentclass[ruled]{article}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose}
\usepackage{subfigure}
\usepackage{color}
\usepackage{url}
\usepackage{algorithm2e}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[unicode=true,
 bookmarks=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=true]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}
%% Special footnote code from the package 'stblftnt.sty'
%% Author: Robin Fairbairns -- Last revised Dec 13 1996
\let\SF@@footnote\footnote
\def\footnote{\ifx\protect\@typeset@protect
    \expandafter\SF@@footnote
  \else
    \expandafter\SF@gobble@opt
  \fi
}
\expandafter\def\csname SF@gobble@opt \endcsname{\@ifnextchar[%]
  \SF@gobble@twobracket
  \@gobble
}
\edef\SF@gobble@opt{\noexpand\protect
  \expandafter\noexpand\csname SF@gobble@opt \endcsname}
\def\SF@gobble@twobracket[#1]#2{}

\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\makeatother

\usepackage{listings}
\lstset{backgroundcolor={\color{white}},
basicstyle={\footnotesize\ttfamily},
breakatwhitespace=false,
breaklines=true,
captionpos=b,
commentstyle={\color{mygreen}},
deletekeywords={...},
escapeinside={\%*}{*)},
extendedchars=true,
frame=shadowbox,
keepspaces=true,
keywordstyle={\color{blue}},
language=Python,
morekeywords={*,...},
numbers=none,
numbersep=5pt,
numberstyle={\tiny\color{mygray}},
rulecolor={\color{black}},
showspaces=false,
showstringspaces=false,
showtabs=false,
stepnumber=1,
stringstyle={\color{mymauve}},
tabsize=2}
\documentclass[UTF8]{article}
\usepackage{CTEX}
\begin{document}
\global\long\def\reals{\mathbf{R}}
 \global\long\def\integers{\mathbf{Z}}
\global\long\def\naturals{\mathbf{N}}
 \global\long\def\rationals{\mathbf{Q}}
\global\long\def\ca{\mathcal{A}}
\global\long\def\cb{\mathcal{B}}
 \global\long\def\cc{\mathcal{C}}
 \global\long\def\cd{\mathcal{D}}
\global\long\def\ce{\mathcal{E}}
\global\long\def\cf{\mathcal{F}}
\global\long\def\cg{\mathcal{G}}
\global\long\def\ch{\mathcal{H}}
\global\long\def\ci{\mathcal{I}}
\global\long\def\cj{\mathcal{J}}
\global\long\def\ck{\mathcal{K}}
\global\long\def\cl{\mathcal{L}}
\global\long\def\cm{\mathcal{M}}
\global\long\def\cn{\mathcal{N}}
\global\long\def\co{\mathcal{O}}
\global\long\def\cp{\mathcal{P}}
\global\long\def\cq{\mathcal{Q}}
\global\long\def\calr{\mathcal{R}}
\global\long\def\cs{\mathcal{S}}
\global\long\def\ct{\mathcal{T}}
\global\long\def\cu{\mathcal{U}}
\global\long\def\cv{\mathcal{V}}
\global\long\def\cw{\mathcal{W}}
\global\long\def\cx{\mathcal{X}}
\global\long\def\cy{\mathcal{Y}}
\global\long\def\cz{\mathcal{Z}}
\global\long\def\ind#1{1(#1)}
\global\long\def\pr{\mathbb{P}}

\global\long\def\ex{\mathbb{E}}
\global\long\def\var{\textrm{Var}}
\global\long\def\cov{\textrm{Cov}}
\global\long\def\sgn{\textrm{sgn}}
\global\long\def\sign{\textrm{sign}}
\global\long\def\kl{\textrm{KL}}
\global\long\def\law{\mathcal{L}}
\global\long\def\eps{\varepsilon}
\global\long\def\convd{\stackrel{d}{\to}}
\global\long\def\eqd{\stackrel{d}{=}}
\global\long\def\del{\nabla}
\global\long\def\loss{\ell}
\global\long\def\tr{\operatorname{tr}}
\global\long\def\trace{\operatorname{trace}}
\global\long\def\diag{\text{diag}}
\global\long\def\rank{\text{rank}}
\global\long\def\linspan{\text{span}}
\global\long\def\proj{\text{Proj}}
\global\long\def\argmax{\operatornamewithlimits{arg\, max}}
\global\long\def\argmin{\operatornamewithlimits{arg\, min}}
\global\long\def\bfx{\mathbf{x}}
\global\long\def\bfy{\mathbf{y}}
\global\long\def\bfl{\mathbf{\lambda}}
\global\long\def\bfm{\mathbf{\mu}}
\global\long\def\calL{\mathcal{L}}
\global\long\def\vw{\boldsymbol{w}}
\global\long\def\vx{\boldsymbol{x}}
\global\long\def\vxi{\boldsymbol{\xi}}
\global\long\def\valpha{\boldsymbol{\alpha}}
\global\long\def\vbeta{\boldsymbol{\beta}}
\global\long\def\vsigma{\boldsymbol{\sigma}}
\global\long\def\vmu{\boldsymbol{\mu}}
\global\long\def\vtheta{\boldsymbol{\theta}}
\global\long\def\vd{\boldsymbol{d}}
\global\long\def\vs{\boldsymbol{s}}
\global\long\def\vt{\boldsymbol{t}}
\global\long\def\vh{\boldsymbol{h}}
\global\long\def\ve{\boldsymbol{e}}
\global\long\def\vf{\boldsymbol{f}}
\global\long\def\vg{\boldsymbol{g}}
\global\long\def\vz{\boldsymbol{z}}
\global\long\def\vk{\boldsymbol{k}}
\global\long\def\va{\boldsymbol{a}}
\global\long\def\vb{\boldsymbol{b}}
\global\long\def\vv{\boldsymbol{v}}
\global\long\def\vy{\boldsymbol{y}}

\title{Note 1}


\section{The Derivative Explanation of Formulas 2.9 to 2.13}
之前我们得到了在线性回归中关于$f(x) = E[Y|X]$的推导。

下面我们再看KNN算法，KNN算法尝试通过训练数据直接通过训练数据完成任务，对于每一个给定点$x$, 我们需要对
所有的输入变量$x_i = x$求其对应$y_i$的均值
\begin{equation}
    \hat{f}(x) = Ave(y_i | x_i \in Neighbor(x))
\end{equation}

在上式中， $Ave$是求平均值的函数， $Neighbor(x)$是一个包含k个与x距离最近
的点的领域($Neighbor(x)$ is a Neighbor containing the k points
in T closest to x)

这里使用了两个近似
\begin{itemize}
    \item 期望被近似为对所有样本点求均值
    \item 一个给定点的条件期望被"松弛"为离该点距离近的区域上的条件期望
\end{itemize}

当样本数据量很大的时候，$x$周围的点会非常靠近$x$.
同时， 根据 mild regularity conditions, 可以证明当$N,k -> \infty$
同时 $\frac{k}{N} -> 0$, 我们可以得到$\hat{f}(x) -> E[Y|X=x]$

同样对于线性回归模型
\begin{align}
    &L(y, X \beta) = (y - X \beta) ^ 2  
    \\ &EPE(\beta) = \int_{x, y} (y - X\beta) ^ 2 
    Pr(X = x, Y = y) dx dy 
    \\ \frac{\partial{EPE(\beta)}}{\partial{\beta}} &= 
    \int_{x}{y} -X^T (y - X \beta) Pr(X = x, Y = y)
    dx dy \notag \\ 
    &=\beta E[X^TX] - E[X^T y]
\end{align}

we set equation (4) to 0, we get $\beta = [E[X^T X]]^{-1} E[X^T y]$
我们将这个表达式与之前的推导式$\hat{\beta} = (X^T X) ^ {-1} 
X^T y$进行对比,最小二乘法的解相当于用训练数据的平均值替换掉了
(4)中的期望

    所以，k-nearest neighbor和最小二乘法都是通过求平均来
近似代替条件期望，但是他们在模型的假设上具有很大的不同
\begin{itemize}
  \item 最小二乘法假定$f(x)$能被一个全局的线性函数较好的拟合
  \item k-nearest neighbor假定$f(x)$能被一个locally 
  constant function较好的近似
\end{itemize}

如果我们使用$L_1$损失函数代替$L_2$, 在这种情况下，解答是
条件中位数(conditional median)

\begin{equation}
  \hat{f}(x) = median(Y|X=x) 
\end{equation}

如果输出变量是一个分类变量呢？我们依然可以使用同一个范式进行处理，除了
使用一个不同的损失函数。一个估计变量$\hat{G}$，损失函数可以被表示为
一个$K \times K$ 的矩阵$L$，其中$L$的对角元素为0，非对角元素为非负
整数，其中$L(k, l)$ 是将原本是$G_l$的类错分为$G_k$类所付出的代价。
同样我们可以先求出expected prediction error:

\begin{align}
  EPE = E[L(G, \hat{G}(X))]
\end{align}

同样，上式的期望依赖于联合概率分布$Pr(G, X)$

\begin{align}
  EPE &= \int_{i=1}^{k} \int_{j=1}^{n} L(G_i, \hat{G}(x_j)) 
  Pr(G_k = m, X = x_j) \\
  &= E_X \int_{k=1}^{K} L(G_k, \hat{G}(X)) Pr(G_k|X)
\end{align}

可以注意到，期望里面的部分始终非负，为了最小化整个期望，我们可以最小化
期望里面的部分

\begin{align}
  \hat{G}(x) &= \argmin_{g \in G} \int_{k = 1}^{K} L(G_k, g)
  Pr(G_k | X = x) \\ 
  &= \argmin_{g \in G} [1 - Pr(g|X=x)]
\end{align}

(10)式可以改写为$\hat{G}(X) = G_k \ if \ Pr(G_k|X=x) = \max_{g \in G}
Pr(g | X = x)$

这个合理的结果为成为Bayes classifier.

\section{高维问题的局部方法}
我们已经证明了两种在预测预测问题中的学习策略：稳定但是带有偏差的线性模型，
不太稳定但是具有较小偏差的k-最近邻估计。

那么我们可能会认为,只要给予足够大的训练数据，k-nearest算法理论上就能够逼近最优解(条件期望)，
但是这种方法在高维情况下就会失败

下面我们将要讨论维数灾难(curse of dimensionality)。主要有两点
\begin{itemize}
  \item 如果我们想使用k-nearest算法来获得neighbor中的点，其与整个空间中的点的比例为
  r，由于空间与线段的关系，想要获得一定比例的数据，我们需要更长的线段来满足这一条件，
  比如在一个边长为1的十维的超立方体中，我们想要获得$1\%$的数据，需要付出0.63的边长。 
  \item 另外，如果在一个高维空间中进行稀疏取样，会使得所有的样本点离样本的某一边很近
\end{itemize}

\section{统计模型，监督学习和函数逼近}
我们的目标在于找到一个对$f(x)$的有效逼近$\hat{f}(x)$，能够有效的表示出
输入和输出之间的映射关系。从之前的章节可以知道，对于变量连续的情况下，
回归函数$f(x) = E(Y | X = x)$。最近邻算法可以被看作是对这种条件期望的
一种直接估计。但是我们看到了，至少在两种情况下，该方法会失败
\begin{itemize}
  \item 如果输入空间的维数太高，会使得最近邻不需要离目标点距离近，导致较大的误差。
  \item If special structure is known to exist, this can be used to reduce both the 
  bias and the variance of the estimates. (不懂) 
\end{itemize}




\end{document}
